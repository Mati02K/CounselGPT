There are 3 components for the model :-

1.LORA :- Main thing which gives domain knowledge to our chatbot and makes it Legal relevant. Train a small layer
which would affect the entire model and make it legal relevant. This is done on separate dataset. We will get adapters
as output. We can also convert it into a model. this should be done on a normal model not a quantized one.

2. RAG :- RAG is to provide extra context to this model, lets say our LORA model is trained upon California laws in
whole and we can RAG with driving laws and show it to user with context. Context is done through vector DB
where we can get top k searches and provide them as context in our prompt

3. Quantization :- WE will combine the adapater and normal model into giving us the 16 or 32 bit precision. 
This large bit will be quantized into 8 or 4 bit.



This dataset looks promising :- https://huggingface.co/datasets/ArchitRastogi/USLawQA
This is based on scraping the https://uscode.house.gov/ website



Chain-of-thought (COT) = model writing its thinking steps.
Why use it?

Because legal interpretation is multi-step reasoning, and CoT teaches the model how to think about:

definitions

clauses

rights

obligations

exceptions

Which directly improves the accuracy of your final legal chatbot.


Just for better Chain Of Thought and understanding the text we are improving it


Use this :- https://arxiv.org/abs/2308.11462  (LegalBench)
and MMLU for evaluation

Using this for Nautilus :-

https://nrp.ai/documentation/userdocs/start/getting-started/

few learnings

How did I stop the GPU Pool to save money :-
gcloud container node-pools update gpu-pool   --cluster counselgpt-cluster   --zone us-west1-a   --enable-autoscaling   --min-nodes=0   --max-nodes=3
gcloud container clusters resize counselgpt-cluster   --node-pool=gpu-pool   --num-nodes=0   --zone=us-west1-a


when I come back just use this :-
gcloud container clusters resize counselgpt-cluster \
  --node-pool=gpu-pool \
  --num-nodes=1 \
  --zone=us-west1-a


k6 run loadtest.js -e API_URL=$API_URL_NAUTILUS > results/loadtest.txt
k6 run spiketest.js -e API_URL=$API_URL_NAUTILUS > results/spiketest.txt
k6 run soaktest.js -e API_URL=$API_URL_NAUTILUS > results/soaktest.txt
k6 run stresstest.js -e API_URL=$API_URL_NAUTILUS > results/stresstest.txt
k6 run endurance.js -e API_URL=$API_URL_NAUTILUS > results/endurance.txt

Check Nautilus resources :- https://nrp.ai/viz/resources/
Could be Prometheus URL In Nautilus Grafana :- http://counselgpt-prometheus.cse239fall2025.svc.cluster.local:9090

How to check what GPU I have. Exec into it :-
kubectl exec -it <POD_NAME> -n cse239fall2025 -- nvidia-smi

watch HPA :-
kubectl get hpa -n cse239fall2025 -w

reason why GPU based scaling was avoided on Nautilus :-
Nautilus does NOT provide Prometheus Adapter.
You cannot install cluster-wide components in Nautilus.

So this approach is not possible unless the admins install it (they wonâ€™t).


GPU's we have at Nautilus :-
ChatGPT said:

L40 (48 GB, Ada Lovelace): Modern datacenter-class GPU with very strong tensor performance and large VRAM, excellent for high-throughput LLM inference.

RTX 2080 Ti (11 GB, Turing): Older consumer GPU with limited VRAM and slower tensor cores, suitable for light inference but much weaker than L40


You can include test assumptions (like how much model should handle)
Compare against SOTA LLM's
Create graph for all test
Show improvement pre and post redis
think about using GPU count for http_req_failed
include tech debt
compare price and performance
talk about future vLLM
think about cooldown in HPA.
put all in table
talk about GFlops and GPU's being Used.
Understand how all metrics in dashboard are derived especially tokens