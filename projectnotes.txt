There are 3 components for the model :-

1.LORA :- Main thing which gives domain knowledge to our chatbot and makes it Legal relevant. Train a small layer
which would affect the entire model and make it legal relevant. This is done on separate dataset. We will get adapters
as output. We can also convert it into a model. this should be done on a normal model not a quantized one.

2. RAG :- RAG is to provide extra context to this model, lets say our LORA model is trained upon California laws in
whole and we can RAG with driving laws and show it to user with context. Context is done through vector DB
where we can get top k searches and provide them as context in our prompt

3. Quantization :- WE will combine the adapater and normal model into giving us the 16 or 32 bit precision. 
This large bit will be quantized into 8 or 4 bit.



This dataset looks promising :- https://huggingface.co/datasets/ArchitRastogi/USLawQA
This is based on scraping the https://uscode.house.gov/ website



Chain-of-thought (COT) = model writing its thinking steps.
Why use it?

Because legal interpretation is multi-step reasoning, and CoT teaches the model how to think about:

definitions

clauses

rights

obligations

exceptions

Which directly improves the accuracy of your final legal chatbot.


Just for better Chain Of Thought and understanding the text we are improving it


Use this :- https://arxiv.org/abs/2308.11462  (LegalBench)
and MMLU for evaluation

Using this for Nautilus :-

https://nrp.ai/documentation/userdocs/start/getting-started/

few learnings

How did I stop the GPU Pool to save money :-
gcloud container node-pools update gpu-pool   --cluster counselgpt-cluster   --zone us-west1-a   --enable-autoscaling   --min-nodes=0   --max-nodes=3
gcloud container clusters resize counselgpt-cluster   --node-pool=gpu-pool   --num-nodes=0   --zone=us-west1-a


when I come back just use this :-
gcloud container clusters resize counselgpt-cluster \
  --node-pool=gpu-pool \
  --num-nodes=1 \
  --zone=us-west1-a


k6 run loadtest.js -e API_URL=$API_URL_NAUTILUS > results/loadtest.txt
k6 run spiketest.js -e API_URL=$API_URL_NAUTILUS > results/spiketest.txt
k6 run soaktest.js -e API_URL=$API_URL_NAUTILUS > results/soaktest.txt
k6 run stresstest.js -e API_URL=$API_URL_NAUTILUS > results/stresstest.txt
k6 run endurance.js -e API_URL=$API_URL_NAUTILUS > results/endurance.txt

Check Nautilus resources :- https://nrp.ai/viz/resources/
Could be Prometheus URL In Nautilus Grafana :- http://counselgpt-prometheus.cse239fall2025.svc.cluster.local:9090

How to check what GPU I have. Exec into it :-
kubectl exec -it <POD_NAME> -n cse239fall2025 -- nvidia-smi

watch HPA :-
kubectl get hpa -n cse239fall2025 -w

reason why GPU based scaling was avoided on Nautilus :-
Nautilus does NOT provide Prometheus Adapter.
You cannot install cluster-wide components in Nautilus.

So this approach is not possible unless the admins install it (they won’t).


GPU's we have at Nautilus :-
ChatGPT said:

L40 (48 GB, Ada Lovelace): Modern datacenter-class GPU with very strong tensor performance and large VRAM, excellent for high-throughput LLM inference.

RTX 2080 Ti (11 GB, Turing): Older consumer GPU with limited VRAM and slower tensor cores, suitable for light inference but much weaker than L40


You can include test assumptions (like how much model should handle)
Compare against SOTA LLM's
Create graph for all test
Show improvement pre and post redis
think about using GPU count for http_req_failed
include tech debt
compare price and performance
talk about future vLLM
think about cooldown in HPA.
put all in table
talk about GFlops and GPU's being Used.
Understand how all metrics in dashboard are derived especially tokens

GCP Grafana :- https://34.111.194.27.nip.io/grafana/login
Nautilus own Grafana :- https://grafana-mathesh.nrp-nautilus.io/d/cf69ukbzmuw3kf/counselgpt-clean-llm-metrics-dashboard?orgId=1&from=now-5m&to=now
Nautilus cloud Grafana :- https://grafana.nrp-nautilus.io/d/dRG9q0Ymz/k8s-compute-resources-namespace-gpus?var-namespace=cse239fall2025&orgId=1&from=now-5m&to=now&timezone=browser&refresh=1d&var-Filters=

Tests planned :-
1. Qwen GPU Acceleration true without cache on normal prompts on all 4 tests, (Load, Stress, Soak, Spike and endurance) 
2. llama (4 bit) GPU Acceleration true without cache on normal prompts on all 4 tests, (Load, Stress, Soak, Spike and endurance) 
3. Qwen with no GPU Acceleration (false) without cache on normal prompts on load test alone.
4. Llama with no GPU Acceleration (false) without cache on normal prompts on load test alone. 
5. Qwen GPU Acceleration true with cache on similar prompts on all 4 tests, (Load, Stress, Soak, Spike and endurance) 
6. llama (4 bit) GPU Acceleration true with cache on similar prompts on all 4 tests, (Load, Stress, Soak, Spike and endurance) 
8. Qwen GPU Acceleration true without cache on large resoning prompts on load tests. 
9. llama (4 bit) GPU Acceleration true without cache on large resoning prompts on load tests.


Clear Cache req :-
curl -X POST https://counselgpt-mathesh.nrp-nautilus.io/cache/clear
curl -X POST https://34.111.194.27.nip.io/cache/clear


NOtes during testing :-
1. Soak Fails in GCP
2. Spike also fails in GCP
MAX VUS supported by Nautilius is roughly 25
Llama fails for same Max VUS in LLama in stress test --> Need to check why
Similar test -> Load can be considered initial few queries where latency is that much but after that it drops down to ms.
Logs seems suprisingly stale for some reason, however no issues in code. 
And when restarted it is proper. Some memory issues or it could be hanged.
I highly suspect there is a huge bug in the way we load model.

Chart intepretations :-
1. GCP failed at 8 VUS at 1 min with 3% failed rate. We can say allowed error rate was 5%
  We can serve at most 1 RPS in nautilus 
  We can say our VUS was around 37 or 38 with close 1% failure rate


In small queries we are looking at 2.43 seconds 
In large reasoing we are looking at 0.3 seconds

Why Qwen can be faster even at higher bit width

Qwen2.5 architecture is more optimized
Newer attention, rotary, activation, and KV-cache patterns → fewer FLOPs per token.

Qwen has better training and tokenizer efficiency
Fewer tokens needed for same output → faster end-to-end latency.

Your model is running on GPU layers
When most layers are on GPU, the difference between 4-bit and 8-bit becomes small.

llama.cpp Q4_K_M is not as fast as it looks
Q4_K_M compresses well but can be slower at runtime because of:

more complex dequant math

additional dequant compute per layer

poor GPU util on some cards

Your Qwen LoRA adapter may be lightweight
Little overhead added.


For final observations

1. HPA did help but not much and model load in HPA is a big issue

Complete Nautilus Readme  
State of the Art addition in Readme
CPU Utilization Chart
Cost calculation