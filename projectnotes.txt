There are 3 components for the model :-

1.LORA :- Main thing which gives domain knowledge to our chatbot and makes it Legal relevant. Train a small layer
which would affect the entire model and make it legal relevant. This is done on separate dataset. We will get adapters
as output. We can also convert it into a model. this should be done on a normal model not a quantized one.

2. RAG :- RAG is to provide extra context to this model, lets say our LORA model is trained upon California laws in
whole and we can RAG with driving laws and show it to user with context. Context is done through vector DB
where we can get top k searches and provide them as context in our prompt

3. Quantization :- WE will combine the adapater and normal model into giving us the 16 or 32 bit precision. 
This large bit will be quantized into 8 or 4 bit.



This dataset looks promising :- https://huggingface.co/datasets/ArchitRastogi/USLawQA
This is based on scraping the https://uscode.house.gov/ website



Chain-of-thought (COT) = model writing its thinking steps.
Why use it?

Because legal interpretation is multi-step reasoning, and CoT teaches the model how to think about:

definitions

clauses

rights

obligations

exceptions

Which directly improves the accuracy of your final legal chatbot.


Just for better Chain Of Thought and understanding the text we are improving it


Use this :- https://arxiv.org/abs/2308.11462  (LegalBench)
and MMLU for evaluation

Using this for Nautilus :-

https://nrp.ai/documentation/userdocs/start/getting-started/

few learnings

How did I stop the GPU Pool to save money :-
gcloud container node-pools update gpu-pool   --cluster counselgpt-cluster   --zone us-west1-a   --enable-autoscaling   --min-nodes=0   --max-nodes=3
gcloud container clusters resize counselgpt-cluster   --node-pool=gpu-pool   --num-nodes=0   --zone=us-west1-a


when I come back just use this :-
gcloud container clusters resize counselgpt-cluster \
  --node-pool=gpu-pool \
  --num-nodes=1 \
  --zone=us-west1-a


k6 run loadtest.js -e API_URL=$API_URL_NAUTILUS > results/loadtest.txt
k6 run soaktest.js -e API_URL=$API_URL_NAUTILUS > results/soaktest.txt
k6 run soaktest.js -e API_URL=$API_URL_NAUTILUS > results/soaktest.txt
k6 run stresstest.js -e API_URL=$API_URL_NAUTILUS > results/stresstest.txt

Check Nautilus resources :- https://nrp.ai/viz/resources/
Could be Prometheus URL In Nautilus Grafana :- http://prometheus.cse239fall2025.svc.cluster.local:9090

GPU's we have at Nautilus :-
ChatGPT said:

L40 (48 GB, Ada Lovelace): Modern datacenter-class GPU with very strong tensor performance and large VRAM, excellent for high-throughput LLM inference.

RTX 2080 Ti (11 GB, Turing): Older consumer GPU with limited VRAM and slower tensor cores, suitable for light inference but much weaker than L40.