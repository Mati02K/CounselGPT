apiVersion: apps/v1
kind: Deployment
metadata:
  name: counselgpt-api-mthiruma
  namespace: cse239fall2025

spec:
  replicas: 1   # HPA will scale it later

  selector:
    matchLabels:
      app: counselgpt-api-mthiruma

  template:
    metadata:
      labels:
        app: counselgpt-api-mthiruma

    spec:

      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

      affinity:
        nodeAffinity:

          # -------------------------------------------------------
          # HARD constraints: MUST be GPU node + CUDA >= 12.4
          # -------------------------------------------------------
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/cuda.runtime.major
                operator: In
                values: ["12"]
              - key: nvidia.com/cuda.runtime.minor
                operator: Gt
                values: ["3"]

              # GPU models allowed (names EXACT from Nautilus)
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  - NVIDIA-L40
                  - NVIDIA-L4
                  - Tesla-V100-SXM2-32GB
                  - Tesla-V100-SXM2-16GB
                  - Tesla-V100-PCIe-16GB

          # -------------------------------------------------------
          # SOFT preferences: Rank best â†’ worst
          # -------------------------------------------------------
          preferredDuringSchedulingIgnoredDuringExecution:

          - weight: 100
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: ["NVIDIA-L40"]

          - weight: 90
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: ["Tesla-V100-SXM2-32GB"]

          - weight: 80
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: ["Tesla-V100-SXM2-16GB"]

          - weight: 70
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: ["Tesla-V100-PCIe-16GB"]

          - weight: 60
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values: ["NVIDIA-L4"]

      containers:
      - name: counselgpt-api
        image: mathesh0208/counselgptapi:v16
        imagePullPolicy: IfNotPresent

        ports:
        - containerPort: 8000

        resources:
          limits:
            cpu: "2"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "1"
            memory: "4Gi"
            nvidia.com/gpu: "1"

        env:
        - name: QWEN_MODEL_PATH
          value: "/models/qwen/Qwen2.5-7B-Instruct-Q8_0.gguf"
        - name: QWEN_LORA_PATH
          value: "/models/qwen/legal_lora_adapter_only_25k.gguf"
        - name: QWEN_GPU_LAYERS
          value: "-1"
        - name: LLAMA_MODEL_PATH
          value: "/models/llama/llama-2-7b-chat.Q4_K_M.gguf"
        - name: LLAMA_GPU_LAYERS
          value: "-1"
        - name: LLM_N_THREADS
          value: "8"
        - name: REDIS_URL
          value: "redis://redis-counselgpt-mthiruma:6379"
        - name: EMBEDDING_URL
          value: "http://counselgpt-embeddings-mthiruma.cse239fall2025.svc.cluster.local:8000"
        - name: USE_SEMANTIC_CACHE
          value: "true"
        - name: SEMANTIC_CACHE_THRESHOLD
          value: "0.85"

        volumeMounts:
        - name: model-volume
          mountPath: /models

        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      volumes:
      - name: model-volume
        persistentVolumeClaim:
          claimName: counselgpt-models-pvc-mthiruma
