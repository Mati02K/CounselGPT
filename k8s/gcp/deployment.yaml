apiVersion: apps/v1
kind: Deployment
metadata:
  name: counselgpt-api
  labels:
    app: counselgpt-api

spec:
  replicas: 1
  strategy:
    type: Recreate  # Delete old pod before creating new one (good for single replica with GPU)
  selector:
    matchLabels:
      app: counselgpt-api

  template:
    metadata:
      labels:
        app: counselgpt-api

    spec:
      # Run ONLY on GPU nodes
      nodeSelector:
        accelerator: nvidia-l4

      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

      # -----------------------------
      # InitContainer = Download models from GCS (only if missing)
      # -----------------------------
      initContainers:
      - name: download-models
        image: google/cloud-sdk:slim
        command:
        - sh
        - -c
        - |
          echo "[INIT] Checking for existing models..."
          mkdir -p /models/llama
          mkdir -p /models/qwen

          # Check and download LLaMA model
          if [ ! -f /models/llama/llama-2-7b-chat.Q4_K_M.gguf ]; then
            echo "[INIT] Downloading LLaMA model..."
            gsutil cp gs://counselgpt-models/llama-2-7b-chat.Q4_K_M.gguf /models/llama/
            echo "[INIT] LLaMA model downloaded."
          else
            echo "[INIT] LLaMA model already exists, skipping download."
          fi

          # Check and download Qwen base model
          if [ ! -f /models/qwen/Qwen2.5-7B-Instruct-Q8_0.gguf ]; then
            echo "[INIT] Downloading Qwen base model..."
            gsutil cp gs://counselgpt-models/Qwen2.5-7B-Instruct-Q8_0.gguf /models/qwen/
            echo "[INIT] Qwen base model downloaded."
          else
            echo "[INIT] Qwen base model already exists, skipping download."
          fi

          # Check and download Qwen LoRA adapter
          if [ ! -f /models/qwen/legal_lora_adapter_only_25k.gguf ]; then
            echo "[INIT] Downloading Qwen LoRA adapter..."
            gsutil cp gs://counselgpt-models/legal_lora_adapter_only_25k.gguf /models/qwen/
            echo "[INIT] Qwen LoRA adapter downloaded."
          else
            echo "[INIT] Qwen LoRA adapter already exists, skipping download."
          fi

          echo "[INIT] All models ready!"
          ls -lh /models/qwen/
          ls -lh /models/llama/

        volumeMounts:
        - name: model-storage
          mountPath: /models

      # -----------------------------
      # Main Inference Container
      # -----------------------------
      containers:
      - name: counselgpt-api
        image: REPLACE_IMAGE
        imagePullPolicy: IfNotPresent

        resources:
          limits:
            cpu: "4"
            memory: "8Gi"
            ephemeral-storage: "30Gi"   # GGUF models are large
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: "4Gi"
            ephemeral-storage: "20Gi"

        env:
        # Qwen Model Paths (LoRA always ON)
        - name: QWEN_MODEL_PATH
          value: "/models/qwen/Qwen2.5-7B-Instruct-Q8_0.gguf"

        - name: QWEN_LORA_PATH
          value: "/models/qwen/legal_lora_adapter_only_25k.gguf"

        - name: QWEN_GPU_LAYERS
          value: "999"

        # LLaMA Model Path
        - name: LLAMA_MODEL_PATH
          value: "/models/llama/llama-2-7b-chat.Q4_K_M.gguf"

        - name: LLAMA_GPU_LAYERS
          value: "35"

        # CPU inference fallback
        - name: LLM_N_THREADS
          value: "8"

        # Redis
        - name: REDIS_URL
          value: "redis://counselgpt-redis:6379"

        ports:
        - name: http
          containerPort: 8000

        volumeMounts:
        - name: model-storage
          mountPath: /models

      # -----------------------------
      # Volumes - Persistent storage for models
      # -----------------------------
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: counselgpt-models
