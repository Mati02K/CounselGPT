apiVersion: apps/v1
kind: Deployment
metadata:
  name: counselgpt-api-gpu
  labels:
    app: counselgpt-api
    tier: gpu

spec:
  replicas: 1  # Single GPU pod (expensive)
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: counselgpt-api
      tier: gpu

  template:
    metadata:
      labels:
        app: counselgpt-api
        tier: gpu

    spec:
      # Run ONLY on GPU nodes
      nodeSelector:
        accelerator: nvidia-l4

      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"

      # -----------------------------
      # InitContainer = Download models from GCS
      # -----------------------------
      initContainers:
      - name: download-models
        image: google/cloud-sdk:slim
        command:
        - sh
        - -c
        - |
          echo "[INIT] Checking for existing models..."
          mkdir -p /models/llama
          mkdir -p /models/qwen

          # Check and download LLaMA model
          if [ ! -f /models/llama/llama-2-7b-chat.Q4_K_M.gguf ]; then
            echo "[INIT] Downloading LLaMA model..."
            gsutil cp gs://counselgpt-models/llama-2-7b-chat.Q4_K_M.gguf /models/llama/
            echo "[INIT] LLaMA model downloaded."
          else
            echo "[INIT] LLaMA model already exists, skipping download."
          fi

          # Check and download Qwen base model
          if [ ! -f /models/qwen/Qwen2.5-7B-Instruct-Q8_0.gguf ]; then
            echo "[INIT] Downloading Qwen base model..."
            gsutil cp gs://counselgpt-models/Qwen2.5-7B-Instruct-Q8_0.gguf /models/qwen/
            echo "[INIT] Qwen base model downloaded."
          else
            echo "[INIT] Qwen base model already exists, skipping download."
          fi

          # Check and download Qwen LoRA adapter
          if [ ! -f /models/qwen/legal_lora_adapter_only_25k.gguf ]; then
            echo "[INIT] Downloading Qwen LoRA adapter..."
            gsutil cp gs://counselgpt-models/legal_lora_adapter_only_25k.gguf /models/qwen/
            echo "[INIT] Qwen LoRA adapter downloaded."
          else
            echo "[INIT] Qwen LoRA adapter already exists, skipping download."
          fi

          echo "[INIT] All models ready!"
          ls -lh /models/qwen/
          ls -lh /models/llama/

        volumeMounts:
        - name: model-storage
          mountPath: /models

      # -----------------------------
      # Main Inference Container (GPU)
      # -----------------------------
      containers:
      - name: counselgpt-api
        image: us-central1-docker.pkg.dev/applied-syntax-479504-j6/counselgpt-image/counselgptapi:e14e6be
        imagePullPolicy: IfNotPresent

        resources:
          limits:
            cpu: "4"
            memory: "8Gi"
            ephemeral-storage: "30Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: "4Gi"
            ephemeral-storage: "20Gi"
            nvidia.com/gpu: "1"

        env:
        # Qwen Model Paths (LoRA always ON)
        - name: QWEN_MODEL_PATH
          value: "/models/qwen/Qwen2.5-7B-Instruct-Q8_0.gguf"

        - name: QWEN_LORA_PATH
          value: "/models/qwen/legal_lora_adapter_only_25k.gguf"

        - name: QWEN_GPU_LAYERS
          value: "-1"

        # LLaMA Model Path
        - name: LLAMA_MODEL_PATH
          value: "/models/llama/llama-2-7b-chat.Q4_K_M.gguf"

        - name: LLAMA_GPU_LAYERS
          value: "-1"

        # CPU inference fallback
        - name: LLM_N_THREADS
          value: "8"

        # Redis
        - name: REDIS_URL
          value: "redis://counselgpt-redis:6379"

        ports:
        - name: http
          containerPort: 8000

        # Health checks
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # Allow up to 5 minutes for models to load
          successThreshold: 1

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1

        volumeMounts:
        - name: model-storage
          mountPath: /models

      # -----------------------------
      # Volumes
      # -----------------------------
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: counselgpt

